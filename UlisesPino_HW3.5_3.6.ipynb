{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzX80ZMRxlyAoqodrWCGjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPino25/MAT422/blob/main/UlisesPino_HW3.5_3.6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.5 and 3.6\n",
        "\n",
        "Important Concepts\n",
        "1.   3.5 K-means\n",
        "2.   3.6 Vector Machine\n",
        "\n",
        "## **3.5 K-means**\n",
        "*k*-means clustering is a popular method of vector quantization that aims to partition n observations and k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. k-means clustering minimizes within cluster variances (squared Euclidiean distances), but not regular Euclidean distances. Which k-means general converge quickly to a local optimum, the problem is computationally difficult (NP-hard). This is shown in Figure 3.12\n",
        "\n",
        "Given a $(x_1,x_2,...,x_n)$ where each observation is a *d*-dimensional real vector, k-means clustering aims to minimize the within-cluster sum of squares (**WCSS**) (i.e. variance), the squared distance of each vector from its centroid summed over all vectors:\n",
        "\n",
        "$$WCSS_i = \\sum_{x ϵ S_i} ||x-\\mu(S_i)||^2,$$\n",
        "\n",
        "where $\\mu (S_i)$ is the mean of points in $S_i$,\n",
        "\n",
        "$$\\mu(S)=\\frac{1}{|S|}\\sum_{x\\epsilon S_i }x.$$\n",
        "\n",
        "The objective is to find:\n",
        "\n",
        "$$arg \\hspace{2mm}min_S \\sum_{i=1}{WCSS_i}.$$\n",
        "\n",
        "*K*-means Clustering Algorithm:\n",
        "\n",
        "1.   Clusters the data into k\n",
        "2.   Select k points at random as cluster centers.\n",
        "3.   Assign objects to their closest cluster center according to the Euclidean\n",
        " distance function.\n",
        "4.   Calculate the centroid or mean of all objects in each cluster.\n",
        "5.   Repeat steps 2,3 and 4 until the same points are assigned to each cluster in consecutive rounds.\n",
        "\n",
        "We now show that *k*-means converges by proving that $\\Sigma_{i=1}^k WCSS_i$ monotonically decreases in each iteration. First, $\\Sigma_{i=1}^k WCSS_i$ decreases in the reassignment step since each vector is assigned to the closest centroid, so the distace it contributes to $\\Sigma_{i=1}^k WCSS_i$ decreases. Second, it decreases in the recomputation step because the new centroid is the vector **v** for which WCSS$_i$ reaches its minimum.\n",
        "\n",
        "$$WCSS_i(v)= \\sum_{x=(x_j)\\epsilon S_i} \\hspace{1 mm} x_j$$\n",
        "\n",
        "which is the componentwise definition of the centroid. Thus, we minimize WCSS$_i$ when the old centroid is replaced with the new centroid. The sum of the WCSS$_i$, must then also decrease during recomputation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TkrY0S_Nksbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.6 Support Vector Machine**\n",
        "\n",
        "Support-vector machines (SVMs) are supervised learning models in machine\n",
        "learning, which aim to analyze data for classification and regression analysis.\n",
        "Given a set of training examples, each marked as belonging to one of\n",
        "two categories, an SVM training algorithm builds a model that assigns new\n",
        "examples to one category or the other. The objective of the support vector\n",
        "machine algorithm is to find a hyperplane in a high dimensional space of the\n",
        "number of features that distinctly classifies the data points. An SVM maps\n",
        "training examples to points in space so as to maximize the width of the gap\n",
        "between the two categories. Predictions of new data are based on which side of the gap they fall.\n",
        "\n",
        "As shown in the Figure 3.13, **W**e are given a training dataset of *n* points of the form\n",
        "\n",
        "$$(x_1,y_1),...,(x_n,y_n)$$\n",
        "\n",
        ", where the y$_i$ are either 1 or -1, each indicating the class to which the point $x_i$ belongs. Each $x_i$ is a *p*-dimensiona real vector. We want to maximize the margin distance of hyperplanes that divides the group of points $x_i$ for which y$_i=1$ from the group of points for which y$_i=-1$. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
        "\n",
        "A hyperplane can be written as the set of points **x** satisfying\n",
        "\n",
        "$$w^T x - b=0$$\n",
        "\n",
        "where **w** is the normal vector to the hyperplane. If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum margin hyperplane is the hyperplane that lies halfway between them as in Figure 3.13. We are interested in two regions: anything on or above this boundary is one of class, with label 1 and anything on or below this boundary is the of the other class, with label -1. The two hyperplanes can be respectively described by the equations\n",
        "\n",
        "$$w^T x - b=1,$$\n",
        "\n",
        "and\n",
        "\n",
        "$$w^T x - b=-1.$$\n",
        "\n",
        "We wish all data points to fall into the margin, which can be expressed as $i$ either\n",
        "\n",
        "$$w^Tx_i-b \\ge 1, if \\hspace{2 mm} y_i=1,$$\n",
        "\n",
        "or\n",
        "\n",
        "$$w^Tx_i-b \\le -1, if \\hspace{2 mm} y_i=-1,$$\n",
        "\n",
        "Together the two constraints that each data point must lie on the correct side of the margin, can be rewritten as\n",
        "\n",
        "$$y_i(w^Tx_i-b)\\ge 1, for \\hspace{1 mm} all \\hspace{1mm} 1  \\le i \\le n.$$\n",
        "\n",
        "We can put this together to get the optimization problem. The goal of the optimization then is to minimize\n",
        "\n",
        "$$\\min_{w,b} \\biggl \\langle \\lambda \\|w\\|^2 + \\frac{1}{n}\\sum_{i=1}^n\\max\\{0,1-y_i(\\langle w,x_i \\rangle - b )\\} \\biggl \\rangle$$\n",
        "\n",
        "which minimizes $\\|w\\|$ subject to $y_i(w^T x_i − b) ≥ 1, for \\hspace {1mm} all \\hspace{1mm} 1 ≤ i ≤ n.$ The first term above is called the regularization term which arises directly from the\n",
        "margin. The parameter $\\lambda$ adjusts the trade-off between increasing the margin\n",
        "size and ensuring that xi lie on the correct side of the margin while we choose\n",
        "the distance of two hyperplanes to be $2/\\|w\\|.$ In principle, the unconstrained optimization problem can be directly solved with gradient descent methods. Because this function is convex in the **w** we\n",
        "can easily apply a gradient descent method to find the minimum.\n"
      ],
      "metadata": {
        "id": "eXn6zpix0eW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### K-means\n",
        "## import all the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "## Create blob dataset\n",
        "ds = make_blobs(n_samples = 200,\n",
        "                centers=4,\n",
        "                num_features=2,\n",
        "                cluster_standev = 1.6,\n",
        "                random_state = 50)\n",
        "\n",
        "## Convert it to an Array\n",
        "ourpoints = ds[0]\n",
        "\n",
        "## Create kmeans objects\n",
        "kmns = KMeans(num_clusters=4)\n",
        "\n",
        "## Fit Kmeans object into dataset\n",
        "kmns.fit(ourpoints)\n",
        "\n",
        "## Plot points in scatterplot\n",
        "plt.scatter(ds[0][:,0], ds[0][:,1])\n",
        "\n",
        "## Find the cluster centroids\n",
        "Clusters = kmns.cluster_centers_\n",
        "\n",
        "## Show the cluster centroids\n",
        "print(Clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "nn2XtKmHREwc",
        "outputId": "46924860-d2e9-4a9f-b1e4-3ead303af9ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7b1a7f08a6b1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## Create blob dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m ds = make_blobs(n_samples = 200, \n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mcenters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: make_blobs() got an unexpected keyword argument 'num_features'"
          ]
        }
      ]
    }
  ]
}