{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGZAb69kzJdFgDFLaOle1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPino25/MAT422/blob/main/UlisesPino_HW3.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4 Logistic Regression\n",
        "\n",
        "Important Concepts:\n",
        "\n",
        "\n",
        "1.   Linear Regression\n"
      ],
      "metadata": {
        "id": "WCNZmjqcywap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Logistic Regression?**\n",
        "\n",
        "Logistic Regression is a model that in its basic form uses a logistic function to model a binary dependent variable. It can be extended to several classes of events such as classification of images. In this section, we illustrate the use of gradient descent on binary classification by logsitic regression.\n",
        "\n",
        "Given the input data is the form $\\{$($a_i,b_i):i=1,...,n$}$\\}$ where $a_i \\epsilon \\hspace{1 mm}\\mathbb{R^d}$ are $b_i \\epsilon \\hspace{1 mm} \\{0,1\\}$ is the label. As before we use a matrix representation: $A \\hspace{1 mm} \\epsilon \\hspace{1 mm} \\mathbb{R^{nxd}}$ has rows $a^T_j, j = 1,...,n$ and **b** $= (b_1,...,b_n)^T \\epsilon \\hspace{1 mm}\\{0,1\\}^n.$ We wish to find a function of the features that approximates the probability of the label 1. For this purpose, we model that logit function of the probability of label 1 as a linear function of the features. Figure 3.11 is the graph of the logit function.\n",
        "\n",
        "For **x,** $a \\hspace{1 mm} \\epsilon \\hspace{1 mm} \\mathbb{R}^d,$ let $p(\\alpha;$**x**$)$ be the probability of the output to be 1, we define\n",
        "$$log \\frac{p(\\alpha;x)}{1-p(\\alpha;x)}=\\alpha^Tx.$$\n",
        "\n",
        "Here $\\alpha^Tx=\\sum x_i\\alpha_i$ can be viewed as a regression problem which seeks the best paramters(**x**) with given data($\\alpha$). Rearranging this expression gives\n",
        "\n",
        "$$p(\\alpha;x)=\\sigma (\\alpha^Tx),$$\n",
        "\n",
        "where the sigmoid function is\n",
        "\n",
        "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n",
        "\n",
        "for $t \\hspace{1 mm} \\epsilon \\hspace{1 mm} \\mathbb{R}.$ To maximize the likelihood of the data, we assume the labels are independent given the features, which is given by\n",
        "\n",
        "$$L(x;A,b)=-\\frac{1}{n}\\prod_{i=1}^n p(\\alpha_i;x)^{b_i}(1-p(\\alpha_i;x))^{1-b_i}.$$\n",
        "\n",
        "Taking a logarithm, multiplying by -1/n and substituting the sigmoid function, we want to minimize the cross-entropy loss.\n",
        "\n",
        "$$l(x;A,b)=-\\frac{1}{n}\\sum_{i=1}^n b_i log(\\sigma(\\alpha^Tx))-\\frac{1}{n}\\sum_{i=1}^n (1-b_i) log(1-\\sigma(\\alpha^Tx))$$\n",
        "\n",
        "That is, we solve\n",
        "\n",
        "$$\\min_{x\\epsilon\\mathbb{R}^d}l(x;A,b).$$\n",
        "\n",
        "To use gradient descent, we need to compute the gradient of $l$. We use the Chain Rule and first compute the derivative of $\\sigma$ which is\n",
        "$$\\sigma'(t)=\\frac{e^{-t}}{(1+e^{-t})^2}=\\frac{1}{1+e^{-t}}\\Biggl(1-\\frac{1}{1+e^{-t}}\\Biggl)=\\sigma(t)(1-\\sigma(t)).$$\n",
        "\n",
        "It follows that $\\sigma(t)$ satisfies the logistic differential equation. It arises in a variety of applications, including the modeling of population dynamics. Here it will be a convenient way to compute the gradient. Indeed observe that by the Chain Rule\n",
        "\n",
        "$$\\nabla_x \\sigma(\\alpha^Tx)=\\sigma(\\alpha^Tx)(1-\\sigma(\\alpha^Tx))\\alpha,$$\n",
        "\n",
        "where we use a subscript **x** to make it clear that the gradient is with respect to **x**.\n",
        "\n"
      ],
      "metadata": {
        "id": "tqgGkOMhzT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Example of Logistic Regression\n",
        "\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "x = np.arange(10).reshape(-1,1)\n",
        "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "x = sm.add_constant(x)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "model = sm.Logit(y, x)\n",
        "result = model.fit(method='newton')\n",
        "result.params\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E3HGrP1Q7PP",
        "outputId": "2d5147ec-e017-4f93-9713-15125fd8fa9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [1. 1.]\n",
            " [1. 2.]\n",
            " [1. 3.]\n",
            " [1. 4.]\n",
            " [1. 5.]\n",
            " [1. 6.]\n",
            " [1. 7.]\n",
            " [1. 8.]\n",
            " [1. 9.]]\n",
            "[0 1 0 0 1 1 1 1 1 1]\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.350471\n",
            "         Iterations 7\n",
            "<statsmodels.discrete.discrete_model.BinaryResultsWrapper object at 0x7b906666ada0>\n"
          ]
        }
      ]
    }
  ]
}