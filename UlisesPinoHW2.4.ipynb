{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeZrRfK4ZzIg0+RNq/UJQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPino25/MAT422/blob/main/UlisesPinoHW2.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KtZCT49E1PD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Maximum Likelihood Estimation\n",
        "\n",
        "### Important Concepts\n",
        "\n",
        "\n",
        "1.   2.4.1 MLE for random samples\n",
        "2.   2.4.2 Linear Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "Bo4nVMQYFtXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4.1 MLE for Random Samples**\n",
        "\n",
        "Maximum likelihood estimation (MLE) is an effective approach of estimating the parameters of a probability distribution through maximizing a likelihood function. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. the logic of maximum likelihood is both intuitive and flexible. As a result, the method has become a dominant means of statistical inference\n",
        "\n",
        "Definition 2.4.1 Let $X_1, X_2,..., X_n$ have joint pmf or pdf\n",
        "$$f(x_1,x_2,...,x_n; \\theta_1,...,\\theta_m)$$\n",
        "where the parameters $\\theta_1,...,\\theta_m$ have unknown values. When $x_1,...,x_n$ are the observed sample values and (2.4.1) is regarded as a function of $\\theta_1,...,\\theta_m,$ it is called the the likelihood function, so that\n",
        "$$f(x_1,...,x_n;\\hat\\theta_1,...,\\hat\\theta_m) \\ge f(x_1,...,x_n;\\theta_1,...,\\theta_m)$$ for all $\\theta_1,...,\\theta_m$\n",
        "\n",
        "When the $X_{i}$'s are substituted in place of the $x_i$ 's, the maximum likelihood estimators result.\n",
        "\n",
        "Example 2.3. Let $X_1,...,X_2$ be a random sample from a normal distribution. The likelihood function is\n",
        "$$f(x_1,...,x_n;\\mu,\\sigma^2)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x_1-\\mu)^2/(2\\sigma^2)}.....\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x_n-\\mu)^2/(2\\sigma^2)}$$\n",
        "\n",
        "$$=\\Bigl(\\frac{1}{2\\pi\\sigma^2}\\Bigl)^{n/2}e^{-\\sum(x_i-\\mu^2)/(2\\sigma^2)}$$\n",
        "\n",
        "So\n",
        "\n",
        "$$\\ln[f(x_1,...,x_n;\\mu,\\sigma^2)]=-\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum(x_i-\\mu)^2$$\n",
        "\n",
        "To find the maximizing values of $\\mu$ and $\\sigma^2,$ we must take the partial derivatives of $\\ln(f)$ with respect to $\\mu,$ we have\n",
        "$$\\frac{\\partial\\ln[f(x_1,...,x_n;\\mu,\\sigma^2)]}{\\partial\\mu}=-\\frac{1}{\\sigma^2}\\sum(x_i-\\mu)$$\n",
        "\n",
        "Equating the derivative zero and solving for $\\mu$ result in the following\n",
        "$$\\hat\\mu=\\frac{1}{n}\\sum x_i$$\n",
        "\n",
        "Similarly, taking derivative with respect to $\\sigma^2,$\n",
        "$$\\frac{\\partial\\ln[f(x_1,...,x_n;\\mu,\\sigma^2)]}{\\partial\\sigma^2}=-\\frac{n}{2\\sigma^2}+\\frac{1}{2\\sigma^4}\\sum(x_i-\\mu)^2$$\n",
        "\n",
        "and\n",
        "$$\\sigma^2=\\frac{\\sum(x_i-\\mu)^2}{n}$$\n",
        "\n",
        "Therefore, the resulting mle's are\n",
        "$$\\hat\\mu = \\bar{X} \\hspace{1 cm} \\sigma^2 = \\frac{\\sum(X_i-\\bar{X})^2}{n}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "uj1N4mYEG2jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4.2 Linear Regression**\n",
        "\n",
        "Given input data points ${(x_i,y_i)}^n_{i=1}$, we seek an affine function to fit the data and each $x_i=(x_{i1},....,x_{ip}).$ The common approach involves finding coefficients $B_j,j=1....,p's$ that minimize the criterion\n",
        "\n",
        "$$\\sum_{i=1}^n(y_i-\\hat{y_i})^2.$$\n",
        "\n",
        "where\n",
        "$$\\hat{y_i}=\\beta_0+\\beta_1x_{i1}+...+\\beta_px_{ip}$$\n",
        "\n",
        "Now we wish to discuss it from a probablistic point of view by the maximum likelihood estimation. Consider that we have $n$ points, each of which is drawn in an independent and identically distributed (i.i.d) way from the normal distribution. For a given, $\\mu,\\sigma^2,$ the probability of those $n$ points being drawn define the likelihood function, which are just the multiplication of $n$ normal probability density functions (pdf) (because they are independent).\n",
        "\n",
        "$$P(\\mu |y)=\\prod_{i=1}^n(P_y(y_i|\\mu,\\sigma^2))=\\prod_{i=1}^n\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}} \\hspace{1 cm} (2.4.2)$$\n",
        "\n",
        "Now we undestand the variable\n",
        "\n",
        "$$y_i=\\hat{y_i}+\\varepsilon,$$\n",
        "\n",
        "where $\\varepsilon \\sim N(0,\\sigma^2).$ Thus, $y_i$ is a normal variable with mean as a linear function of $x$ and a fixed standard deviation:\n"
      ],
      "metadata": {
        "id": "0HsfZuHQB02N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Maximum Likelihood Estimation\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "T = np.array([[2, 4, 6, 8, 10]])\n",
        "\n",
        "A = np.shape(T)[1]\n",
        "\n",
        "print(\"Here is our sample data: \\n\", T)\n",
        "print(\"The amount of data we have in our NumpyArray is: \", A)\n",
        "\n",
        "# This is to calculate our mean\n",
        "MU = np.array([[2, 4, 6, 4, 6]])\n",
        "\n",
        "# This is to calculate our standard deviation\n",
        "Sigma = np.array([[2, 4, 4, 6, 6]])\n",
        "print(\"\\nOur mean is: \", MU)\n",
        "print(\"Our standard deviation is: \", Sigma)\n",
        "\n",
        "# This displays our data for our mean, standard deviation,\n",
        "# and logL\n",
        "print(\"MU Sigma logL\")\n",
        "for i in range(5):\n",
        "  logL = -np.sum(np.square(T - MU[0,i])/(2*np.square(Sigma[0,i])))\n",
        "  - (0.5*A*np.log10(2*math.pi)) - A*np.log10(Sigma[0,i])\n",
        "  print(MU[0, i], Sigma[0,i], np.round(logL,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfQMW9TmxEVy",
        "outputId": "361f3281-fe0c-4bd6-e6e9-4fdf380f56ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is our sample data: \n",
            " [[ 2  4  6  8 10]]\n",
            "The amount of data we have in our NumpyArray is:  5\n",
            "\n",
            "Our mean is:  [[2 4 6 4 6]]\n",
            "Our standard deviation is:  [[2 4 4 6 6]]\n",
            "MU Sigma logL\n",
            "2 2 -15.0\n",
            "4 4 -1.875\n",
            "6 4 -1.25\n",
            "4 6 -0.833\n",
            "6 6 -0.556\n"
          ]
        }
      ]
    }
  ]
}