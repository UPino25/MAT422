{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+4K0w8eOtGCO27QrqHSt+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPino25/MAT422/blob/main/UlisesPino_HW3.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 Unconstrained Optimization\n",
        "\n",
        "Important Concepts:\n",
        "\n",
        "\n",
        "1.   3.3.1 Necessary and Sufficient Conditions of Local Minimizers\n",
        "2.   3.3.2 Convexity and global minimizers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4vRNR2VrpwFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.1 Necessary and Sufficient Conditions of Local Minimizers**\n",
        "\n",
        "We will be interested in unconstrained optimization of the form:\n",
        "$$\\min_{x \\in \\mathbb{R}}f(x),$$\n",
        "where $f:\\mathbb{R}^d \\to \\mathbb{R}.$ In this subsection, we defined several notions of solution and derive characterizations. Ideally, we would would like to find a global minimizer to the optimization problem above.\n",
        "\n",
        "Definition 3.3.1 (Global Minimizer) Let $f: \\mathbb{R}^d \\to \\mathbb{R}.$ The point **x**$^* \\in \\mathbb{R}$ is a global minimizer of $f$ over $\\mathbb{R}^d$ if\n",
        "$$f(x) \\ge f(x^*), \\hspace{4 mm} \\forall x \\in \\mathbb{R}^d.$$\n",
        "\n",
        "Often it is difficult to find a global minimizer unless some special structure is present. Therefore weaker notions of solution have been introduced. The relationship between global minimizer and local minimizer is shown in Figure 3.8.\n",
        "\n",
        "Definition 3.3.2 (Local minimizer) Let $f: \\mathbb{R}^d \\to \\mathbb{R}.$ The point **x**$^* \\in \\mathbb{R}$ is a local minimizer of $f$ over $\\mathbb{R}^d$ if there is $\\delta > 0$ such that\n",
        "$$f(x) \\ge f(x^*), \\hspace{4 mm} \\forall x \\in B_\\delta(x^*)\\backslash \\{x\\}.$$\n",
        "\n",
        "If the inequality is strict, we say that **x**$^*$ is a strict local minimizer.\n",
        "\n",
        "**x**$^*$ is a local minimizer if there is open ball around **x**$^*$ where it attains the minimum value. The difference between global and local minimizers is illustrated in Figure 3.8. We will characterize local minimizers in terms of the gradient and Hessian of the function. We first need to define what a descent direction is, which generalizes the case when the derivative of a one dimensional function is negative.\n",
        "\n",
        "Definition 3.3.3 (Descent Direction) Let $f:\\mathbb R^d \\to \\mathbb R.$ A vector **v** is a descent direction for $f$ at x$_0$ if there is $\\alpha^* > 0$ such that\n",
        "$$f(x_0+\\alpha v)<f(x_0), \\hspace{4 mm} \\forall\\alpha \\in (0,\\alpha^*).$$\n",
        "\n",
        "In the continuously differentiable case, the directional derivative gives a criterion for decent directions.\n",
        "\n",
        "Lemma 3.3.4 (Descent Direction and Directional Derivative) Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be continously differentiable at x$_0.$ A vector **v** is a descent direction for $f$ at **x$_0$** if\n",
        "$$\\frac{\\partial f(x_0)}{\\partial v}=\\nabla f(x_0)^Tv < 0$$\n",
        "that is, if the directional derivative of $f$ at **x$_0$** in the direction **v** is negative."
      ],
      "metadata": {
        "id": "kzX472gvrlA-"
      }
    }
  ]
}