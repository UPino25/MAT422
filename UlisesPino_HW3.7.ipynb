{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSd3sIQmOrSpgjJ0fZK8ZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPino25/MAT422/blob/main/UlisesPino_HW3.7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "Important Concepts:\n",
        "\n",
        "1.   3.7.1 Mathematical Formulation\n",
        "2.   3.7.2 Activiation Formuluation\n",
        "\n"
      ],
      "metadata": {
        "id": "f7MJQS7i-cDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.7.1 Mathematical Formulation**\n",
        "\n",
        "Figure 3.14 shows the simplest network. $x_1$ and $x_2$ are inputs from the left, and a forecast output on the right, $\\hat{y}$, which is modified by the activation function $\\sigma(z)$ chosen in advance.\n",
        "\n",
        "$$\\hat{y}=\\sigma(z)=\\sigma(w_1a_1 + w_2a_2+b).$$\n",
        "\n",
        "In neural networks, the weights, the $w_i$, and the bias, $b$, will be found numerically in order to best fit our forecast output with our given data.\n",
        "\n",
        "For a general neural network as in Figure 3.15 is a neural network. A general network may has hundreds or thousands of nodes. It demonstrates the inputs and outputs of neural networks. The input units receive various forms and structures of information based on an internal weighting system, and the neural network attempts to learn about the information presented to produce one output report. Specifically, it adjust its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as supervised learning.\n",
        "\n",
        "Now we formulate mathematical notation for a neural network. In Figure 3.16, we look at how values of layer $l$ are determined from layer $l-1$ and $l$. Also notice that the general node in the left-hand layer is labelled $j$ and one in the right-hand layer, layer $l$, is labelled $j'$. We want to calculate what value goes into the $j^{th}$ node of the $l^{th}$ layer. First multiply the value $a_j^{(l-1)}$ in the j^th node of the previous, $(l-1)^{th},$ layer by the parameter $w_{j,j'}^{(l)}$ and then add another parameter $b_{j'}^{(l)}$. Then we add up all of these for every node in layer $l-1$. Let\n",
        "\n",
        "$$z_{j'}^{(l)}=\\sum_{j=1}^{J_{l-1}}w_{j,j'}^{(l)}a^{l-1}+b_{j'}^{(l)},$$\n",
        "\n",
        "where $J_I$ means the number of nodes in layer $l$. For a given activation function, $\\sigma$, we end up with the following expression for the values in the next layer,\n",
        "\n",
        "$$a_{j'}^{(l)} = \\sigma(z_{j'}^{(l)}).$$\n",
        "\n",
        "In matrix form,\n",
        "\n",
        "$$z(l)=W^{(l)}a^{(l-1)}+b^{(l)}$$,\n",
        "\n",
        "with the matrix $W^{(l)}$ containing all the multiplicative parameters, i.e. the weights $w_{j,j'}^{(l)}$ and b$^{(l)}$ is the bias. The bias is just the constant in the linear transformation\n",
        "\n",
        "$$a^{(l)}=\\sigma(z^{(l)})=\\sigma(W^{(l)}a^{(l-1)}+b^{(l)})$$"
      ],
      "metadata": {
        "id": "PhunlUPRA-zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.7.2 Activation Functions**\n",
        "\n",
        "In neural networks, the activation function of a node abstracts the output of\n",
        "that node given an input or set of inputs for specific purposes for example,\n",
        "classification. In biological neural networks, the activation function may represent\n",
        "an electrical signal wether if the neuron fires. We use $\\sigma$ to represent\n",
        "the activation functions. It will be the same for all nodes in a layer.\n",
        "\n",
        "\n",
        "$$a^{(l)}=\\sigma(z^{(l)})=\\sigma(W^{(l)}a^{(l-1)}+b^{(l)})$$"
      ],
      "metadata": {
        "id": "eoPk5qn7ZlHn"
      }
    }
  ]
}